{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hosting containers on AWS for teaching","text":"<p>Scripts to deploy multiple docker containers simultaneously for teaching. Below you will find the documentation. Are you new? Have a look at the tutorial</p> <p>A note on security</p> <p>The connections between the participant and the container will be through http, and are therefore not encrypted. Therefore:</p> <ul> <li>No sensitive data in the hosted container</li> <li>Do not let participants choose their own password</li> <li>Be careful with downloading executable files</li> <li>Do not re-use the IP of the host machine</li> <li>(If possible) restrict the IP from which to approach the host machine</li> </ul> <p>We are currently developing scripts and docs on how to host containers with a reverse proxy over https. You can find it here: https://github.com/GeertvanGeest/AWS-docker-nginx</p>"},{"location":"#preparation","title":"Preparation","text":"<p>Start an AWS EC2 instance with an Ubuntu AMI. If you are new to this, here is a good place to get started. </p> <p>Here\u2019s a repository that automates the start of an EC2 instance with <code>terraform</code>. </p> <p>If docker is not pre-installed in your chosen AMI (it is pre-installed in e.g. the Ubuntu deep learning base AMI), install docker on the instance:</p> <pre><code>curl https://get.docker.com | sh\nsudo usermod -a -G docker ubuntu # ubuntu is the user with root access\nsudo service docker start\n</code></pre> <p>You can add the above code to the code run at initialisation. Otherwise, logout and login again to be able use <code>docker</code> without <code>sudo</code>.</p> <p>After that, clone this repository:</p> <pre><code>git clone https://github.com/GeertvanGeest/AWS-docker.git\n</code></pre>"},{"location":"#generate-credentials","title":"Generate credentials","text":"<p>You can generate credentials from a comma separated list of users, with two columns: first name and last name. Do not use column names. Here\u2019s an example:</p> <pre><code>Jan,de Wandelaar\nPiet,Kopstoot\nJoop,Zoetemelk\n</code></pre> <p>Run the script <code>generate_credentials.sh</code> like this (use <code>-l</code> to specify the user list):</p> <pre><code>./generate_credentials \\\n-l examples/user_list_credentials.txt \\\n-o ./credentials\n-p 9001\n-a 18.192.64.150\n</code></pre> <p>The option <code>-o</code> specifies an output directory in which the following files are created:</p> <ul> <li><code>input_docker_start.txt</code>: A file that serves as input to deploy the docker containers on the server</li> <li><code>user_info.txt</code>: A file with user names, passwords and links that can be used to communicate credentials to the participants</li> </ul> <p>The option <code>-p</code> is used to generate an individual port for each user. Ports will be assigned in an increasing number from <code>-p</code> to each user. So, in the example above, the first user gets port 9001, the second 9002, the third 9003, etc. Be aware that port 9000 and 10000 are reserved for the admin containers!</p> <p>The option <code>-a</code> specifies the address on which the container will be hosted. This is used to generate the address per user. If you have already started your instance (or if you have an elastic IP), specify the public IPv4 address here. </p>"},{"location":"#deploying-containers","title":"Deploying containers","text":"<p>The output generated by <code>generate_credentials</code> consists of two files. One of them is called <code>input_docker_start.txt</code> and  should look like this (if you wish, you can also choose to generate this yourself):</p> <pre><code>9001    jdewandelaar    OZDRqwMRmkjKzM48v+I=\n9002    pkopstoot   YTnSh6SmhsVUe+aC2HY=\n9003    jzoetemelk  LadwVbiYY4rH0S5TjeI=\n</code></pre> <p>Each line will be used to start up a container hosted on the specified port and accessible with the specified password (third column). Once deployed, the jupyter notebook or rstudio server will be available through <code>[HOST IP]:[PORT]</code>. If you want to have both rstudio server and jupyter notebook running on the same instance, you can generate two tab-delimited files (one for rstudio and one for jupyter) and give them the same passwords for convenience. Note that each container uses a single port, so the files should contain different ports!</p>"},{"location":"#deploy-containers-based-on-jupyter-notebook","title":"Deploy containers based on jupyter notebook","text":"<p>Prepare an image that you want to use for the course. This image should be based on a jupyter notebook container, e.g. jupyter/base-notebook, and should be available from <code>dockerhub</code>.</p> <p>Run the script <code>run_jupyter_notebooks</code> on the server:</p> <pre><code>run_jupyter_notebooks \\\n-i jupyter/base-notebook \\\n-u examples/credentials_jupyter/input_docker_start.txt \\\n-p test1234\n</code></pre> <p>Here, <code>-i</code> is the image tag, <code>-u</code> is the user list as generated by <code>./generate_credentials.sh</code>, and <code>-p</code> is the password for the admin container. No username is required to log on to a jupyter notebook.</p> <p>To access the admin container, go to <code>[HOST IP]:10000</code></p>"},{"location":"#deploy-containers-based-on-rstudio-server","title":"Deploy containers based on Rstudio server","text":"<p>Prepare an image that you want to use for the course. This image should be based on a rocker image, e.g. rocker/rstudio, and should be available from <code>dockerhub</code>.</p> <p>Run the script <code>run_rstudio_server</code> on the server:</p> <pre><code>run_rstudio_server \\\n-i rocker/rstudio \\\n-u examples/credentials_rstudio/input_docker_start.txt \\\n-p test1234\n</code></pre> <p>See above for the meaning of the options.</p> <p>The username to log on to rstudio server is <code>rstudio</code>.</p> <p>To access the admin container, go to <code>[HOST IP]:9000</code></p>"},{"location":"#deploy-containers-based-on-vscode-server","title":"Deploy containers based on vscode server","text":"<p>Prepare an image that you want to use for the course. This image should be based on a image linuxserver/code-server image, and should be available from <code>dockerhub</code>.</p> <p>In the docker file you can install code-server extensions with <code>/usr/local/bin/install-extension</code>. </p> <p>Run the script <code>run_vscode_server</code> on the server:</p> <pre><code>run_vscode_server \\\n-i linuxserver/code-server \\\n-u examples/credentials_vscode/input_docker_start.txt \\\n-p test1234\n</code></pre> <p>See above for the meaning of the options.</p> <p>To access the admin container, go to <code>[HOST IP]:7000</code></p>"},{"location":"#restricting-resource-usage","title":"Restricting resource usage","text":"<p>To prevent overcommitment of the server, it can be convenient to restrict resource usage per participant. You can do that with the options <code>-c</code> and <code>-m</code>, which are passed to the arguments <code>--cpus</code> and <code>--memory</code> of <code>docker run</code>. Use it like this:</p> <pre><code>run_rstudio_server \\\n-i rocker/rstudio \\\n-u examples/credentials_rstudio/input_docker_start.txt \\\n-p test1234 \\\n-c 2 \\\n-m 4g\n</code></pre> <p>Resulting in a hard limit of 2 cpu and 4 Gb of memory for each user. By default these are 2 cpu and 16 Gb of memory. These restrictions are not applied to the admin container.</p>"},{"location":"#container-volume-infrastructure","title":"Container &amp; volume infrastructure","text":"<p>There are three volumes mounted to each container:</p> <ul> <li>The volume <code>data</code> is mounted to <code>/data</code>. This volume is meant to harbour read-only data (e.g. raw data). </li> <li>The volume <code>group_work</code> is mounted to <code>/group_work</code>. The group volume is meant as a shared directory, where everybody can read and write.</li> <li>Each user has a personal volume, named after the username (output of <code>generate_credentials</code>). This volume is mounted to <code>/home/rstudio/workdir/</code> for rstudio, <code>/home/jovyan/workdir</code> for jupyter, and <code>/config/project</code> for vscode. </li> </ul> <p>Below you can find an example of the container infrastructure. Blue squares are containers, yellow are volumes. Arrows indicate accessibility. </p> <p></p>"},{"location":"#how-to-use-admin-privileges","title":"How to use admin privileges","text":"<p>The admin container (i.e. with sudo rights) is available from port 10000 for the jupyter containers and 9000 for the rstudio containers. The regular users at the ports specified in the tab-delimited text file.</p> <p>You can check out a user volume with <code>mount_user_volume.sh</code>:</p> <pre><code>./mount_user_volume.sh user01\n</code></pre> <p>This will create an ubuntu container directly accessing the home directory of user01. As an alternative to this ubuntu container, you can mount the user volume to any other container.</p>"},{"location":"#stopping-services","title":"Stopping services","text":"<p>You can stop all services (containers and volumes) with the script <code>stop_services.sh</code>.</p>"},{"location":"#setting-up-a-backup","title":"Setting up a backup","text":"<p>With the script <code>backup_s3.sh</code> you can sync files from the docker volumes to s3. It will sync the shared volume <code>group_work</code> and the invidual user volumes. In order to run the script, first configure AWS cli on the server:</p> <pre><code>aws configure\n</code></pre> <p>More info about configuring AWS cli here. </p> <p>After that, we can specify a cronjob, to sync these files regularly. The script <code>scripts/backup_s3_cronjob.sh</code> calls <code>backup_s3.sh</code> and can be used in your cronjob. To do this, first edit <code>scripts/backup_s3_cronjob.sh</code>:</p> <pre><code>#!/usr/bin/env bash\n\ncd /home/ubuntu\nAWS-docker/backup_s3 \\\n-u [CREDENTIALS input_docker_start.txt] \\\n-s [EXISTING S3 BUCKET] \\\n-e [DIRECTORY IN THE BUCKET (newly created)] \\\n2&gt;&gt; cronjob.err\n</code></pre> <p>Now run:</p> <pre><code>crontab -e\n</code></pre> <p>And add a cronjob. E.g. for every hour you can add this line (use a full path to the cronjob script):</p> <pre><code>0 * * * * /home/ubuntu/backup_s3_cronjob.sh\n</code></pre>"},{"location":"tutorial/","title":"Tutorial","text":"<p>In this tutorial we will go through the following steps: </p> <ul> <li>start up an EC2 instance</li> <li>generate credentials </li> <li>deploying containers running Rstudio server. </li> </ul> <p>The tutorial assumes that you already have an AWS account. If you haven\u2019t got one, make one here. </p>"},{"location":"tutorial/#video","title":"Video","text":"<p>Note</p> <p>The video below introduces FAIR training in general. The tutorial starts at 27:00. </p>"},{"location":"tutorial/#start-up-an-ec2-instance","title":"Start up an EC2 instance","text":"<p>Let\u2019s start with a small EC2 instance with the following characteristics:</p> <ul> <li>Type <code>t2.micro</code> (free small instance)</li> <li>Running Ubuntu</li> <li>30Gb of disk space</li> <li>Accessible through ports 22 (ssh) and 9000-9050 (web access)</li> </ul> <p>Do this by navigating to the EC2 console, and press the orange button Launch instances. Choose a ubuntu image running on <code>t2.micro</code>. At Key pair select the key pair you want to use (if you already have create one), or generate a new key. If you generate a new key, choose the default settings. It will automatically download the .pem file. After you have downloaded the key file, store it into a secure place, and change the permissions to <code>400</code> (if on Linux/Mac):</p> <pre><code>chmod 400 mykey.pem\n</code></pre> <p>Add a security group rule by clicking Edit at Network settings and clicking Add security group rule. At port range specify <code>9000-9050</code>, and at source <code>0.0.0.0/0</code> (meaning the ports can be approached from anywhere). At Configure storage specify 30 Gb of gp2 storage as Root volume. To launch the instance, now click Launch instance. </p>"},{"location":"tutorial/#login-to-the-instance-and-install-docker","title":"Login to the instance and install docker","text":"<p>To login to the instance, first find the public IPv4 address. To find this IP, go to the EC2 console, select your running instance, and find it at the tab Details. Now that you know your IP, open your local terminal and cd into the directory where you have stored your key file (.pem). After that, use ssh to logon onto the instance:</p> <pre><code>ssh -i mykey.pem ubuntu@[public IP address]\n</code></pre> <p>Now that we have logged on to our instance, we can install Docker. Do this by running the following commands:</p> <pre><code>curl https://get.docker.com | sh\nsudo usermod -a -G docker ubuntu # ubuntu is the user with root access\nsudo service docker start\n</code></pre> <p>Now logout and login again in order to be able to use docker without <code>sudo</code>. </p>"},{"location":"tutorial/#generate-credentials","title":"Generate credentials","text":"<p>Now, clone this repository into your home directory on the instance (<code>/home/ubuntu</code>):</p> <pre><code>git clone https://github.com/sib-swiss/AWS-docker.git\n</code></pre> <p>First, we generate some credentials (link + password). We have prepared a comma delimited file with user information to test inside the repository at <code>examples/user_list_credentials.txt</code>. It\u2019s just comma-delimited file with first names and last names:</p> <pre><code>Jan,de Wandelaar\nPiet,Kopstoot\nJoop,Zoetemelk\n</code></pre> <p>Now that we have the user list ready we can generate the credentials with the script <code>generate_credentials</code>:</p> <pre><code>cd ~/AWS-docker\n# [public IP address] is the public IPv4 address of your instance\n./generate_credentials \\\n-l examples/user_list_credentials.txt \\\n-o credentials \\\n-p 9001 \\\n-a [public IP address]\n</code></pre> <p>The option <code>-o</code> specifies an output directory in which the following files are created:</p> <ul> <li><code>input_docker_start.txt</code>: A file that serves as input to deploy the docker containers on the server</li> <li><code>user_info.txt</code>: A file with user names, passwords and links that can be used to communicate credentials to the participants</li> </ul> <p>The option <code>-p</code> is used to generate an individual port for each user. Ports will be assigned in an increasing number from <code>-p</code> to each user. So, in the example above, the first user gets port 9001, the second 9002, the third 9003, etc. Be aware that port 9000 and 10000 are reserved for the admin containers!</p> <p>The option <code>-a</code> specifies the address on which the container will be hosted. This is used to generate the address per user. </p>"},{"location":"tutorial/#start-an-rstudio-container","title":"Start an Rstudio container","text":"<p>Now that we have created credentials, we can start up three containers for the three users that we originally had in our user list. We will start up an rstudio container with the script <code>run_rstudio_server</code>. </p> <pre><code>./run_rstudio_server \\\n-i rocker/rstudio \\\n-u ./credentials/input_docker_start.txt \\\n-p adminpassword \\\n-m 1g \\\n-c 1\n</code></pre> <p>Here, we provide the image name at <code>-i</code>. This can be any image that is based on a <code>rocker/rstudio</code> image and hosted on docker hub. At <code>-u</code> we provide the credential information that we generated in the previous step. At <code>-p</code> we specify a password that is used to login to the admin container. At <code>-m</code> the memory limit, and <code>-c</code> the cpu limit (1GB with 1 CPU because that\u2019s what we have available at our small instance). </p> <p>This will start up the containers and volumes. Participants can access the containers with the link and password provided in <code>credentials/user_info.txt</code>. The admin container is available at port 9000. For the rstudio container, the username will be rstudio.</p>"},{"location":"tutorial/#stopping-containers","title":"Stopping containers","text":"<p>In order to stop all containers you can run:</p> <pre><code>docker stop $(docker ps -aq)\n</code></pre> <p>After stopping the containers, you can safely stop your instance (by using the EC2 console). With stopping the instance, you can keep your instance state, but you won\u2019t be charged for the instance (only for the disk space). </p> <p>In order to stop, remove all containers and volumes you can the <code>stop_services.sh</code> script in the <code>scripts</code> directory:</p> <pre><code>cd ~/AWS-docker/scripts\n./stop_services.sh\n</code></pre> <p>At the end of your class/tutorial, don\u2019t forget to terminate the instance! Do this by navigating to the EC2 console. Select the instance, and click Instance state &gt; Terminate instance. </p>"},{"location":"usage/","title":"How to use the containers","text":"<p>In this chapter there is some general information for people (e.g. teachers/course participants) who are using the hosted containers after they have been started up. </p>"},{"location":"usage/#general-information","title":"General information","text":"<ul> <li>During the course the containers are hosted on an AWS EC2 instance.</li> <li>Each student gets assigned one container of each available type (i.e. Rstudio, jupyter ..)</li> <li>Computational resources are limited per container. By default this is 16G RAM and 4 CPU. This can be increased if needed. The limits are hard, meaning that if they get exceeded (especially RAM), the container will crash and automatically restarted (everything in working memory will be lost). </li> <li>Each container will get a unique port assigned, so a link to an individual container will look like <code>http://1.23.45.678:10034</code>. </li> <li>In the container, you can find the following shared directories:<ul> <li><code>/data</code>: read only, and shared between all running containers. This directory is used to have a single place to store data</li> <li><code>/group_work</code>: read and write enabled for all participants, and shared between all containers. This can be used to share data/scripts between students. This directory can be backed up. </li> <li><code>~/project</code> or <code>~/workdir</code>: read and write enabled, and only shared between containers assigned to the same participant. This directory can be backed up and shared as a tarball at the end of the course. </li> </ul> </li> <li>All directories other than the shared directories only exist within the container. </li> <li>Some firewalls do not support http connections. In that case, use port forwarding instead of using the public IP address. This would require an extra user to set up the port forwarding:<ul> <li>Create a new user on the EC2 instance with public and private keys</li> <li>Set up port forwarding on the local machine with <code>ssh -L 10034:public_ip_address:10034 -i /path/to/private_key.pem user@public_ip_address</code></li> </ul> </li> </ul>"},{"location":"usage/#jupyter-containers","title":"Jupyter containers","text":"<ul> <li>Interaction with the container is either via the terminal, jupyter notebook or python console. </li> <li>Software of individual sessions (e.g. projects) will be in a conda environment. Check which environments are available with <code>conda env list</code>. You can use these environments in two ways:<ul> <li>Using the terminal, e.g. <code>conda activate env_name</code></li> <li>By switching the kernel of jupyter notebooks (e.g. by using Kernel &gt; Change Kernel.. )</li> </ul> </li> </ul>"},{"location":"usage/#rstudio-containers","title":"Rstudio containers","text":"<ul> <li>All R packages will be installed in the system library. </li> <li>Conda environments are installed with the <code>reticulate</code> conda installation. Check which environments are available with <code>conda env list</code> on the terminal.</li> <li>You can use the conda environments in two ways:<ul> <li>Using the terminal, e.g. <code>conda activate env_name</code></li> <li>In <code>R</code> (with <code>reticulate</code>): <code>reticulate::use_condaenv(\"env_name\")</code>. </li> </ul> </li> </ul>"}]}